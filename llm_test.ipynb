{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# установка и импорт библиотек",
   "id": "aaa64136c1c5bb6c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T16:21:03.569851Z",
     "start_time": "2025-03-26T16:20:52.860817Z"
    }
   },
   "cell_type": "code",
   "source": [
    "!pip install -q datasets transformers peft bitsandbytes accelerate trl jsonlines zstandard\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import jsonlines\n",
    "import pandas as pd\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from trl import SFTTrainer\n"
   ],
   "id": "c7427c54ba78c294",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## установка констант и проверка использование cuda",
   "id": "61dbddb3f33c0bbc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T16:21:03.580941Z",
     "start_time": "2025-03-26T16:21:03.575769Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Настройки обучения\n",
    "OUTPUT_DIR = \"qwen1.5-7b-chat-ru-turbo-saiga\"\n",
    "MODEL_NAME = \"Qwen/Qwen1.5-7B-Chat\"\n",
    "MAX_SEQ_LENGTH = 2048\n",
    "LEARNING_RATE = 2e-4\n",
    "NUM_TRAIN_EPOCHS = 3\n",
    "BATCH_SIZE = 2\n",
    "GRADIENT_ACCUMULATION_STEPS = 8\n",
    "\n",
    "# Проверка доступности GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ],
   "id": "b516b880aca2948e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Конвертор исходного датасета в chatml (требуется для qwen1.5-7b)",
   "id": "3b0aba3491000066"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T16:21:08.689698Z",
     "start_time": "2025-03-26T16:21:03.614047Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Импорт и использование функции преобразования\n",
    "def convert_to_chatml(example):\n",
    "    \"\"\"Преобразует пример в формат ChatML для Qwen\"\"\"\n",
    "    formatted_text = \"<|im_start|>system\\nYou are a helpful assistant that accurately answers user queries in Russian.<|im_end|>\\n\"\n",
    "    \n",
    "    for role, content in zip(example['messages']['role'], example['messages']['content']):\n",
    "        # Замена ролей\n",
    "        if role == \"bot\":\n",
    "            role = \"assistant\"\n",
    "        elif role == \"user\":\n",
    "            role = \"user\"\n",
    "            \n",
    "        # Добавление сообщения в ChatML формате\n",
    "        formatted_text += f\"<|im_start|>{role}\\n{content}<|im_end|>\\n\"\n",
    "    \n",
    "    return {\"text\": formatted_text}\n",
    "\n",
    "# Загрузка датасета\n",
    "dataset = load_dataset(\"IlyaGusev/ru_turbo_saiga\", split=\"train\")\n",
    "\n",
    "# Применение преобразования и сбор в массив\n",
    "processed_dataset = dataset.map(convert_to_chatml)\n",
    "formatted_samples = [entry['text'] for entry in processed_dataset]\n",
    "\n",
    "# Пример первого элемента массива\n",
    "print(\"Sample formatted entry:\", formatted_samples[0])\n",
    "\n",
    "# Проверяем размер полученного массива\n",
    "print(f\"Total formatted entries: {len(formatted_samples)}\")\n"
   ],
   "id": "e8c98bfe6fa54683",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample formatted entry: <|im_start|>system\n",
      "You are a helpful assistant that accurately answers user queries in Russian.<|im_end|>\n",
      "<|im_start|>user\n",
      "Мне нужно найти название фильма. В нем детей из детдомов набирают в группу для спец операций. Ты можешь мне помочь?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Конечно. Навскидку я могу предложить несколько фильмов. Но, скорее всего, ты ищешь фильм под названием \"Сиротский батальон\" (англ. \"The Little Bastards\"). Эту картина, снятая в 2003 году, рассказывает о группе детей совершающих различные военные миссии, посланные на линию фронта.<|im_end|>\n",
      "<|im_start|>user\n",
      "Да, спасибо. Извини, не было времени на поиск поисковиком. А есть ли другие фильмы на эту тему?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Конечно. Есть еще один фильм, который может заинтересовать тебя. Он называется \"Солдаты неудачи\" (англ. \"Toy Soldiers\"). В этом фильме подростки из интернатной школы похищаются террористами, и вынуждены сражаться за свою жизнь. Этот фильм был выпущен в 1991 году.<|im_end|>\n",
      "\n",
      "Total formatted entries: 37731\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T16:21:08.727550Z",
     "start_time": "2025-03-26T16:21:08.724146Z"
    }
   },
   "cell_type": "code",
   "source": "formatted_samples[2]",
   "id": "7a34765a50b59940",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|>system\\nYou are a helpful assistant that accurately answers user queries in Russian.<|im_end|>\\n<|im_start|>user\\nЯ хотел бы узнать больше о том, как использовать счётчики для проверки результата.<|im_end|>\\n<|im_start|>assistant\\nКонечно, с удовольствием помогу вам! Счётчики - это инструмент, который позволяет сравнить ожидаемый результат с фактическим. Они используются в тестировании программного обеспечения и различных алгоритмах. Создание корректных счётчиков поможет вам быстро обнаружить ошибки в вашем коде.<|im_end|>\\n<|im_start|>user\\nКак мне создать счётчик?<|im_end|>\\n<|im_start|>assistant\\nВы можете создать простой счётчик, который будет работать с числами. Например, в Python вы можете создать переменную и задать ей начальное значение 0. Далее, внутри цикла или иного блока кода, вы можете увеличивать это значение на 1 каждый раз, когда выполняется определенное условие. В конце выполнения программы вы можете сравнить значение этой переменной с ожидаемым результатом.<|im_end|>\\n<|im_start|>user\\nА что, если я работаю с другими типами данных?<|im_end|>\\n<|im_start|>assistant\\nНекоторые языки программирования могут иметь специальные встроенные счётчики для работы с другими типами данных, например, массивами или строками. Если встроенных функций не предостаточно, вы можете создать свой собственный счётчик, используя классы и методы, которые позволят вам работать с нужными вам типами данных.<|im_end|>\\n'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#  Токенизация данных",
   "id": "559f3d0bff4c2da5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T16:21:35.349783Z",
     "start_time": "2025-03-26T16:21:08.764621Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Инициализация токенизатора\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "\n",
    "# Функция токенизации\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding=True, truncation=True, max_length=2048)\n",
    "\n",
    "# Преобразование ваших данных в формат датасета и токенизация\n",
    "from datasets import Dataset\n",
    "\n",
    "# Создаем Dataset из formatted_samples\n",
    "formatted_dataset = Dataset.from_dict({\"text\": formatted_samples})\n",
    "\n",
    "# Применяем токенизацию\n",
    "tokenized_dataset = formatted_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Разделение на тренировочную и валидационную выборку\n",
    "train_size = int(0.95 * len(tokenized_dataset))\n",
    "train_dataset = tokenized_dataset.select(range(train_size))\n",
    "eval_dataset = tokenized_dataset.select(range(train_size, len(tokenized_dataset)))\n"
   ],
   "id": "86d3e83d43711e39",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 37731/37731 [00:25<00:00, 1497.24 examples/s]\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "5544d9ec4b363d42"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T19:42:32.536963Z",
     "start_time": "2025-03-26T19:42:32.533077Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_dataset\n",
    "# получаем датасет (везде в начале стоит системное сообщение, поэтому в начале все данные одинаковые)"
   ],
   "id": "e26f3e0263056f8c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 35844\n",
       "})"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Загрузка модели + квантизация",
   "id": "476034e24f83e261"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T16:33:07.423291Z",
     "start_time": "2025-03-26T16:23:18.362297Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Настройка квантизации для экономии памяти\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# Загрузка модели с квантизацией\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    quantization_config=bnb_config\n",
    ")"
   ],
   "id": "e1c440d452d7c951",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]C:\\python\\sirius_2025\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Сергей\\.cache\\huggingface\\hub\\models--Qwen--Qwen1.5-7B-Chat. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Fetching 4 files: 100%|██████████| 4/4 [09:07<00:00, 136.98s/it]\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:39<00:00,  9.90s/it]\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# LoRA",
   "id": "cab3e90440495c37"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T16:49:09.343496Z",
     "start_time": "2025-03-26T16:49:08.704964Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Настройка LoRA для эффективного дообучения\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\", \n",
    "        \"v_proj\", \n",
    "        \"o_proj\",\n",
    "        \"gate_proj\", \n",
    "        \"up_proj\", \n",
    "        \"down_proj\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Применение LoRA к модели\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ],
   "id": "eee75547b169dd6b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 39,976,960 || all params: 7,761,301,504 || trainable%: 0.5151\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Подготовка trainer к обучанию",
   "id": "92b57596562eaac0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T16:49:25.931280Z",
     "start_time": "2025-03-26T16:49:23.503617Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Настройка аргументов обучения\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    logging_steps=10,\n",
    "    num_train_epochs=NUM_TRAIN_EPOCHS,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=500,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    do_eval=True,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.05,\n",
    "    optim=\"adamw_torch\",\n",
    "    report_to=\"none\",\n",
    "    fp16=True,\n",
    "    remove_unused_columns=False,\n",
    "    group_by_length=True,\n",
    ")\n",
    "\n",
    "# Инициализация SFT тренера\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    ")"
   ],
   "id": "d875c8a3e2c1e286",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\python\\sirius_2025\\.venv\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Truncating train dataset: 100%|██████████| 35844/35844 [00:00<00:00, 41814.93 examples/s]\n",
      "Truncating eval dataset: 100%|██████████| 1887/1887 [00:00<00:00, 47293.16 examples/s]\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Обучаем",
   "id": "a987d24818cdbe39"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### К сожелению на данном этапе мне пришлось остановится, т.к. обучалось слишком долго =(\n",
    "#### Но я считаю что проделал достойную работу, как минимум я получил ценный опыт"
   ],
   "id": "35ff63efb4c4b161"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "6403a4a1324fab20"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Запуск обучения\n",
    "trainer.train()"
   ],
   "id": "5a24513801a671e3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model.save_pretrained(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "print(\"Training completed and model saved!\")"
   ],
   "id": "7e81324bf71592c",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
