{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# —É—Å—Ç–∞–Ω–æ–≤–∫–∞ –∏ –∏–º–ø–æ—Ä—Ç –±–∏–±–ª–∏–æ—Ç–µ–∫",
   "id": "aaa64136c1c5bb6c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T16:21:03.569851Z",
     "start_time": "2025-03-26T16:20:52.860817Z"
    }
   },
   "cell_type": "code",
   "source": [
    "!pip install -q datasets transformers peft bitsandbytes accelerate trl jsonlines zstandard\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import jsonlines\n",
    "import pandas as pd\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from trl import SFTTrainer\n"
   ],
   "id": "c7427c54ba78c294",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## —É—Å—Ç–∞–Ω–æ–≤–∫–∞ –∫–æ–Ω—Å—Ç–∞–Ω—Ç –∏ –ø—Ä–æ–≤–µ—Ä–∫–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ cuda",
   "id": "61dbddb3f33c0bbc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T16:21:03.580941Z",
     "start_time": "2025-03-26T16:21:03.575769Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –æ–±—É—á–µ–Ω–∏—è\n",
    "OUTPUT_DIR = \"qwen1.5-7b-chat-ru-turbo-saiga\"\n",
    "MODEL_NAME = \"Qwen/Qwen1.5-7B-Chat\"\n",
    "MAX_SEQ_LENGTH = 2048\n",
    "LEARNING_RATE = 2e-4\n",
    "NUM_TRAIN_EPOCHS = 3\n",
    "BATCH_SIZE = 2\n",
    "GRADIENT_ACCUMULATION_STEPS = 8\n",
    "\n",
    "# –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ],
   "id": "b516b880aca2948e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# –ö–æ–Ω–≤–µ—Ä—Ç–æ—Ä –∏—Å—Ö–æ–¥–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ –≤ chatml (—Ç—Ä–µ–±—É–µ—Ç—Å—è –¥–ª—è qwen1.5-7b)",
   "id": "3b0aba3491000066"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T16:21:08.689698Z",
     "start_time": "2025-03-26T16:21:03.614047Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# –ò–º–ø–æ—Ä—Ç –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è\n",
    "def convert_to_chatml(example):\n",
    "    \"\"\"–ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –ø—Ä–∏–º–µ—Ä –≤ —Ñ–æ—Ä–º–∞—Ç ChatML –¥–ª—è Qwen\"\"\"\n",
    "    formatted_text = \"<|im_start|>system\\nYou are a helpful assistant that accurately answers user queries in Russian.<|im_end|>\\n\"\n",
    "    \n",
    "    for role, content in zip(example['messages']['role'], example['messages']['content']):\n",
    "        # –ó–∞–º–µ–Ω–∞ —Ä–æ–ª–µ–π\n",
    "        if role == \"bot\":\n",
    "            role = \"assistant\"\n",
    "        elif role == \"user\":\n",
    "            role = \"user\"\n",
    "            \n",
    "        # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è –≤ ChatML —Ñ–æ—Ä–º–∞—Ç–µ\n",
    "        formatted_text += f\"<|im_start|>{role}\\n{content}<|im_end|>\\n\"\n",
    "    \n",
    "    return {\"text\": formatted_text}\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞\n",
    "dataset = load_dataset(\"IlyaGusev/ru_turbo_saiga\", split=\"train\")\n",
    "\n",
    "# –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –∏ —Å–±–æ—Ä –≤ –º–∞—Å—Å–∏–≤\n",
    "processed_dataset = dataset.map(convert_to_chatml)\n",
    "formatted_samples = [entry['text'] for entry in processed_dataset]\n",
    "\n",
    "# –ü—Ä–∏–º–µ—Ä –ø–µ—Ä–≤–æ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–∞ –º–∞—Å—Å–∏–≤–∞\n",
    "print(\"Sample formatted entry:\", formatted_samples[0])\n",
    "\n",
    "# –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä –ø–æ–ª—É—á–µ–Ω–Ω–æ–≥–æ –º–∞—Å—Å–∏–≤–∞\n",
    "print(f\"Total formatted entries: {len(formatted_samples)}\")\n"
   ],
   "id": "e8c98bfe6fa54683",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample formatted entry: <|im_start|>system\n",
      "You are a helpful assistant that accurately answers user queries in Russian.<|im_end|>\n",
      "<|im_start|>user\n",
      "–ú–Ω–µ –Ω—É–∂–Ω–æ –Ω–∞–π—Ç–∏ –Ω–∞–∑–≤–∞–Ω–∏–µ —Ñ–∏–ª—å–º–∞. –í –Ω–µ–º –¥–µ—Ç–µ–π –∏–∑ –¥–µ—Ç–¥–æ–º–æ–≤ –Ω–∞–±–∏—Ä–∞—é—Ç –≤ –≥—Ä—É–ø–ø—É –¥–ª—è —Å–ø–µ—Ü –æ–ø–µ—Ä–∞—Ü–∏–π. –¢—ã –º–æ–∂–µ—à—å –º–Ω–µ –ø–æ–º–æ—á—å?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "–ö–æ–Ω–µ—á–Ω–æ. –ù–∞–≤—Å–∫–∏–¥–∫—É —è –º–æ–≥—É –ø—Ä–µ–¥–ª–æ–∂–∏—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ñ–∏–ª—å–º–æ–≤. –ù–æ, —Å–∫–æ—Ä–µ–µ –≤—Å–µ–≥–æ, —Ç—ã –∏—â–µ—à—å —Ñ–∏–ª—å–º –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º \"–°–∏—Ä–æ—Ç—Å–∫–∏–π –±–∞—Ç–∞–ª—å–æ–Ω\" (–∞–Ω–≥–ª. \"The Little Bastards\"). –≠—Ç—É –∫–∞—Ä—Ç–∏–Ω–∞, —Å–Ω—è—Ç–∞—è –≤ 2003 –≥–æ–¥—É, —Ä–∞—Å—Å–∫–∞–∑—ã–≤–∞–µ—Ç –æ –≥—Ä—É–ø–ø–µ –¥–µ—Ç–µ–π —Å–æ–≤–µ—Ä—à–∞—é—â–∏—Ö —Ä–∞–∑–ª–∏—á–Ω—ã–µ –≤–æ–µ–Ω–Ω—ã–µ –º–∏—Å—Å–∏–∏, –ø–æ—Å–ª–∞–Ω–Ω—ã–µ –Ω–∞ –ª–∏–Ω–∏—é —Ñ—Ä–æ–Ω—Ç–∞.<|im_end|>\n",
      "<|im_start|>user\n",
      "–î–∞, —Å–ø–∞—Å–∏–±–æ. –ò–∑–≤–∏–Ω–∏, –Ω–µ –±—ã–ª–æ –≤—Ä–µ–º–µ–Ω–∏ –Ω–∞ –ø–æ–∏—Å–∫ –ø–æ–∏—Å–∫–æ–≤–∏–∫–æ–º. –ê –µ—Å—Ç—å –ª–∏ –¥—Ä—É–≥–∏–µ —Ñ–∏–ª—å–º—ã –Ω–∞ —ç—Ç—É —Ç–µ–º—É?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "–ö–æ–Ω–µ—á–Ω–æ. –ï—Å—Ç—å –µ—â–µ –æ–¥–∏–Ω —Ñ–∏–ª—å–º, –∫–æ—Ç–æ—Ä—ã–π –º–æ–∂–µ—Ç –∑–∞–∏–Ω—Ç–µ—Ä–µ—Å–æ–≤–∞—Ç—å —Ç–µ–±—è. –û–Ω –Ω–∞–∑—ã–≤–∞–µ—Ç—Å—è \"–°–æ–ª–¥–∞—Ç—ã –Ω–µ—É–¥–∞—á–∏\" (–∞–Ω–≥–ª. \"Toy Soldiers\"). –í —ç—Ç–æ–º —Ñ–∏–ª—å–º–µ –ø–æ–¥—Ä–æ—Å—Ç–∫–∏ –∏–∑ –∏–Ω—Ç–µ—Ä–Ω–∞—Ç–Ω–æ–π —à–∫–æ–ª—ã –ø–æ—Ö–∏—â–∞—é—Ç—Å—è —Ç–µ—Ä—Ä–æ—Ä–∏—Å—Ç–∞–º–∏, –∏ –≤—ã–Ω—É–∂–¥–µ–Ω—ã —Å—Ä–∞–∂–∞—Ç—å—Å—è –∑–∞ —Å–≤–æ—é –∂–∏–∑–Ω—å. –≠—Ç–æ—Ç —Ñ–∏–ª—å–º –±—ã–ª –≤—ã–ø—É—â–µ–Ω –≤ 1991 –≥–æ–¥—É.<|im_end|>\n",
      "\n",
      "Total formatted entries: 37731\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T16:21:08.727550Z",
     "start_time": "2025-03-26T16:21:08.724146Z"
    }
   },
   "cell_type": "code",
   "source": "formatted_samples[2]",
   "id": "7a34765a50b59940",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|>system\\nYou are a helpful assistant that accurately answers user queries in Russian.<|im_end|>\\n<|im_start|>user\\n–Ø —Ö–æ—Ç–µ–ª –±—ã —É–∑–Ω–∞—Ç—å –±–æ–ª—å—à–µ –æ —Ç–æ–º, –∫–∞–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å—á—ë—Ç—á–∏–∫–∏ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞.<|im_end|>\\n<|im_start|>assistant\\n–ö–æ–Ω–µ—á–Ω–æ, —Å —É–¥–æ–≤–æ–ª—å—Å—Ç–≤–∏–µ–º –ø–æ–º–æ–≥—É –≤–∞–º! –°—á—ë—Ç—á–∏–∫–∏ - —ç—Ç–æ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–∑–≤–æ–ª—è–µ—Ç —Å—Ä–∞–≤–Ω–∏—Ç—å –æ–∂–∏–¥–∞–µ–º—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Å —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏–º. –û–Ω–∏ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –≤ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏ –ø—Ä–æ–≥—Ä–∞–º–º–Ω–æ–≥–æ –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è –∏ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∞–ª–≥–æ—Ä–∏—Ç–º–∞—Ö. –°–æ–∑–¥–∞–Ω–∏–µ –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã—Ö —Å—á—ë—Ç—á–∏–∫–æ–≤ –ø–æ–º–æ–∂–µ—Ç –≤–∞–º –±—ã—Å—Ç—Ä–æ –æ–±–Ω–∞—Ä—É–∂–∏—Ç—å –æ—à–∏–±–∫–∏ –≤ –≤–∞—à–µ–º –∫–æ–¥–µ.<|im_end|>\\n<|im_start|>user\\n–ö–∞–∫ –º–Ω–µ —Å–æ–∑–¥–∞—Ç—å —Å—á—ë—Ç—á–∏–∫?<|im_end|>\\n<|im_start|>assistant\\n–í—ã –º–æ–∂–µ—Ç–µ —Å–æ–∑–¥–∞—Ç—å –ø—Ä–æ—Å—Ç–æ–π —Å—á—ë—Ç—á–∏–∫, –∫–æ—Ç–æ—Ä—ã–π –±—É–¥–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å —Å —á–∏—Å–ª–∞–º–∏. –ù–∞–ø—Ä–∏–º–µ—Ä, –≤ Python –≤—ã –º–æ–∂–µ—Ç–µ —Å–æ–∑–¥–∞—Ç—å –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –∏ –∑–∞–¥–∞—Ç—å –µ–π –Ω–∞—á–∞–ª—å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ 0. –î–∞–ª–µ–µ, –≤–Ω—É—Ç—Ä–∏ —Ü–∏–∫–ª–∞ –∏–ª–∏ –∏–Ω–æ–≥–æ –±–ª–æ–∫–∞ –∫–æ–¥–∞, –≤—ã –º–æ–∂–µ—Ç–µ —É–≤–µ–ª–∏—á–∏–≤–∞—Ç—å —ç—Ç–æ –∑–Ω–∞—á–µ–Ω–∏–µ –Ω–∞ 1 –∫–∞–∂–¥—ã–π —Ä–∞–∑, –∫–æ–≥–¥–∞ –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–µ —É—Å–ª–æ–≤–∏–µ. –í –∫–æ–Ω—Ü–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –ø—Ä–æ–≥—Ä–∞–º–º—ã –≤—ã –º–æ–∂–µ—Ç–µ —Å—Ä–∞–≤–Ω–∏—Ç—å –∑–Ω–∞—á–µ–Ω–∏–µ —ç—Ç–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π —Å –æ–∂–∏–¥–∞–µ–º—ã–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–º.<|im_end|>\\n<|im_start|>user\\n–ê —á—Ç–æ, –µ—Å–ª–∏ —è —Ä–∞–±–æ—Ç–∞—é —Å –¥—Ä—É–≥–∏–º–∏ —Ç–∏–ø–∞–º–∏ –¥–∞–Ω–Ω—ã—Ö?<|im_end|>\\n<|im_start|>assistant\\n–ù–µ–∫–æ—Ç–æ—Ä—ã–µ —è–∑—ã–∫–∏ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è –º–æ–≥—É—Ç –∏–º–µ—Ç—å —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–µ —Å—á—ë—Ç—á–∏–∫–∏ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –¥—Ä—É–≥–∏–º–∏ —Ç–∏–ø–∞–º–∏ –¥–∞–Ω–Ω—ã—Ö, –Ω–∞–ø—Ä–∏–º–µ—Ä, –º–∞—Å—Å–∏–≤–∞–º–∏ –∏–ª–∏ —Å—Ç—Ä–æ–∫–∞–º–∏. –ï—Å–ª–∏ –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π –Ω–µ –ø—Ä–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ, –≤—ã –º–æ–∂–µ—Ç–µ —Å–æ–∑–¥–∞—Ç—å —Å–≤–æ–π —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–π —Å—á—ë—Ç—á–∏–∫, –∏—Å–ø–æ–ª—å–∑—É—è –∫–ª–∞—Å—Å—ã –∏ –º–µ—Ç–æ–¥—ã, –∫–æ—Ç–æ—Ä—ã–µ –ø–æ–∑–≤–æ–ª—è—Ç –≤–∞–º —Ä–∞–±–æ—Ç–∞—Ç—å —Å –Ω—É–∂–Ω—ã–º–∏ –≤–∞–º —Ç–∏–ø–∞–º–∏ –¥–∞–Ω–Ω—ã—Ö.<|im_end|>\\n'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#  –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö",
   "id": "559f3d0bff4c2da5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T16:21:35.349783Z",
     "start_time": "2025-03-26T16:21:08.764621Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "\n",
    "# –§—É–Ω–∫—Ü–∏—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding=True, truncation=True, max_length=2048)\n",
    "\n",
    "# –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤–∞—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö –≤ —Ñ–æ—Ä–º–∞—Ç –¥–∞—Ç–∞—Å–µ—Ç–∞ –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è\n",
    "from datasets import Dataset\n",
    "\n",
    "# –°–æ–∑–¥–∞–µ–º Dataset –∏–∑ formatted_samples\n",
    "formatted_dataset = Dataset.from_dict({\"text\": formatted_samples})\n",
    "\n",
    "# –ü—Ä–∏–º–µ–Ω—è–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—é\n",
    "tokenized_dataset = formatted_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—É—é –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—É—é –≤—ã–±–æ—Ä–∫—É\n",
    "train_size = int(0.95 * len(tokenized_dataset))\n",
    "train_dataset = tokenized_dataset.select(range(train_size))\n",
    "eval_dataset = tokenized_dataset.select(range(train_size, len(tokenized_dataset)))\n"
   ],
   "id": "86d3e83d43711e39",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 37731/37731 [00:25<00:00, 1497.24 examples/s]\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "5544d9ec4b363d42"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T19:42:32.536963Z",
     "start_time": "2025-03-26T19:42:32.533077Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_dataset\n",
    "# –ø–æ–ª—É—á–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç (–≤–µ–∑–¥–µ –≤ –Ω–∞—á–∞–ª–µ —Å—Ç–æ–∏—Ç —Å–∏—Å—Ç–µ–º–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ, –ø–æ—ç—Ç–æ–º—É –≤ –Ω–∞—á–∞–ª–µ –≤—Å–µ –¥–∞–Ω–Ω—ã–µ –æ–¥–∏–Ω–∞–∫–æ–≤—ã–µ)"
   ],
   "id": "e26f3e0263056f8c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 35844\n",
       "})"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ + –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è",
   "id": "476034e24f83e261"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T16:33:07.423291Z",
     "start_time": "2025-03-26T16:23:18.362297Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —Å –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–µ–π\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    quantization_config=bnb_config\n",
    ")"
   ],
   "id": "e1c440d452d7c951",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]C:\\python\\sirius_2025\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\–°–µ—Ä–≥–µ–π\\.cache\\huggingface\\hub\\models--Qwen--Qwen1.5-7B-Chat. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Fetching 4 files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [09:07<00:00, 136.98s/it]\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:39<00:00,  9.90s/it]\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# LoRA",
   "id": "cab3e90440495c37"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T16:49:09.343496Z",
     "start_time": "2025-03-26T16:49:08.704964Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ LoRA –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –¥–æ–æ–±—É—á–µ–Ω–∏—è\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\", \n",
    "        \"v_proj\", \n",
    "        \"o_proj\",\n",
    "        \"gate_proj\", \n",
    "        \"up_proj\", \n",
    "        \"down_proj\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "# –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ LoRA –∫ –º–æ–¥–µ–ª–∏\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ],
   "id": "eee75547b169dd6b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 39,976,960 || all params: 7,761,301,504 || trainable%: 0.5151\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ trainer –∫ –æ–±—É—á–∞–Ω–∏—é",
   "id": "92b57596562eaac0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T16:49:25.931280Z",
     "start_time": "2025-03-26T16:49:23.503617Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤ –æ–±—É—á–µ–Ω–∏—è\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    logging_steps=10,\n",
    "    num_train_epochs=NUM_TRAIN_EPOCHS,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=500,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    do_eval=True,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.05,\n",
    "    optim=\"adamw_torch\",\n",
    "    report_to=\"none\",\n",
    "    fp16=True,\n",
    "    remove_unused_columns=False,\n",
    "    group_by_length=True,\n",
    ")\n",
    "\n",
    "# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è SFT —Ç—Ä–µ–Ω–µ—Ä–∞\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    ")"
   ],
   "id": "d875c8a3e2c1e286",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\python\\sirius_2025\\.venv\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Truncating train dataset: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 35844/35844 [00:00<00:00, 41814.93 examples/s]\n",
      "Truncating eval dataset: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1887/1887 [00:00<00:00, 47293.16 examples/s]\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# –û–±—É—á–∞–µ–º",
   "id": "a987d24818cdbe39"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### –ö —Å–æ–∂–µ–ª–µ–Ω–∏—é –Ω–∞ –¥–∞–Ω–Ω–æ–º —ç—Ç–∞–ø–µ –º–Ω–µ –ø—Ä–∏—à–ª–æ—Å—å –æ—Å—Ç–∞–Ω–æ–≤–∏—Ç—Å—è, —Ç.–∫. –æ–±—É—á–∞–ª–æ—Å—å —Å–ª–∏—à–∫–æ–º –¥–æ–ª–≥–æ =(\n",
    "#### –ù–æ —è —Å—á–∏—Ç–∞—é —á—Ç–æ –ø—Ä–æ–¥–µ–ª–∞–ª –¥–æ—Å—Ç–æ–π–Ω—É—é —Ä–∞–±–æ—Ç—É, –∫–∞–∫ –º–∏–Ω–∏–º—É–º —è –ø–æ–ª—É—á–∏–ª —Ü–µ–Ω–Ω—ã–π –æ–ø—ã—Ç"
   ],
   "id": "35ff63efb4c4b161"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "6403a4a1324fab20"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è\n",
    "trainer.train()"
   ],
   "id": "5a24513801a671e3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model.save_pretrained(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "print(\"Training completed and model saved!\")"
   ],
   "id": "7e81324bf71592c",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
